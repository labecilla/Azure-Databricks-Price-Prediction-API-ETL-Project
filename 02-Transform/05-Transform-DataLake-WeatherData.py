# Databricks notebook source
# MAGIC %md
# MAGIC WeatherData JSON Source File Path : "abfss://bronze@datalakestorageaccountname.dfs.core.windows.net/weather-data/
# MAGIC "
# MAGIC
# MAGIC - <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join" target="_blank">**DataFrame Joins** </a>

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1: Define the Variables to read weather-data ingested in bronze Layer
# MAGIC
# MAGIC 1. Replace <datalakestorageaccountname> with the ADLS account name crated in your account
# MAGIC

# COMMAND ----------

weatherDataSourceLayerName = 'bronze'
weatherDataSourceStorageAccountName = '<datalakestorageaccountname>'
weatherDataSourceFolderName = 'weather-data'

weatherDataSourceFolderPath = f"abfss://{weatherDataSourceLayerName}@{weatherDataSourceStorageAccountName}.dfs.core.windows.net/{weatherDataSourceFolderName}"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2: Create Spark Dataframe For weather-data in Json form stored in bronze layer
# MAGIC
# MAGIC 1. Define Spark Dataframe variable name as weatherDataBronzeDF
# MAGIC 1. Use spark.read.json method to read the source data path defined above using the variable weatherDataSourceFolderPath 
# MAGIC 1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing
# MAGIC

# COMMAND ----------

weatherDataBronzeDF = (spark
                       .read
                       .json(weatherDataSourceFolderPath))

display(weatherDataBronzeDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3: Convert Weathe Date Values in ARRAY format to ROWS Using Explode
# MAGIC
# MAGIC 1. Import all functions from pyspark.sql.functions package
# MAGIC 1. Define New Spark Dataframe variable name as weatherDataDailyDateTransDF
# MAGIC 1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF
# MAGIC 1. First select column is "daily.time" and apply the explode function on this source column and also add alias for exploded values column as "weatherDate"
# MAGIC 1. Along with above explode select the columns "marketName" , "latitude" , "longitude" from source Spark Dataframe
# MAGIC 1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'
# MAGIC 1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing
# MAGIC

# COMMAND ----------

from pyspark.sql.functions import *
weatherDataDailyDateTransDF = (weatherDataBronzeDF
                          .select(
                          explode("daily.time").alias("weatherDate")
                          ,col("marketName")
                          ,col("latitude").alias("latitude")
                          ,col("longitude").alias("longitude")
                          ,monotonically_increasing_id().alias('sequenceId')
                          ))

display(weatherDataDailyDateTransDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4: Convert Maximum Temparature Values in ARRAY format to ROWS Using Explode
# MAGIC
# MAGIC 1. Define New Spark Dataframe variable name as weatherDataMaxTemparatureTransDF
# MAGIC 1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF
# MAGIC 1. First select column is "daily.temperature_2m_max" and apply the explode function on this source column and also add alias for exploded values column as "maximumTemparature"
# MAGIC 1. Along with above explode select the columns "marketName" , "latitude" , "longitude" from source Spark Dataframe
# MAGIC 1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'
# MAGIC 1. Add one more column from the Source Spark Dataframe "daily_units.temperature_2m_max" and provide alias name as "unitOfTemparature"
# MAGIC 1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing

# COMMAND ----------

weatherDataMaxTemparatureTransDF = (weatherDataBronzeDF
                          .select(
                          explode("daily.temperature_2m_max").alias("maximumTemparature")
                          ,col("marketName")
                          ,col("latitude").alias("latitude")
                          ,col("longitude").alias("longitude")
                          ,monotonically_increasing_id().alias('sequenceId')
                          ,col("daily_units.temperature_2m_max").alias("unitOfTemparature")

                          ))

display(weatherDataMaxTemparatureTransDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5: Convert Minimum Temparature Values in ARRAY format to ROWS Using Explode
# MAGIC
# MAGIC 1. Define New Spark Dataframe variable name as weatherDataMinTemparatureTransDF
# MAGIC 1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF
# MAGIC 1. First select column is "daily.temperature_2m_min" and apply the explode function on this source column and also add alias for exploded values column as "minimumTemparature"
# MAGIC 1. Along with above explode select the columns "marketName" , "latitude" , "longitude" from source Spark Dataframe
# MAGIC 1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'
# MAGIC 1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing

# COMMAND ----------

weatherDataMinTemparatureTransDF = (weatherDataBronzeDF
                          .select(
                          explode("daily.temperature_2m_min").alias("minimumTemparature")
                          ,col("marketName")
                          ,col("latitude").alias("latitude")
                          ,col("longitude").alias("longitude")                          
                          ,monotonically_increasing_id().alias('sequenceId')

                          ))

display(weatherDataMinTemparatureTransDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 6: Convert Rain Fall Values in ARRAY format to ROWS Using Explode
# MAGIC
# MAGIC 1. Define New Spark Dataframe variable name as weatherDataRainFallTransDF
# MAGIC 1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF
# MAGIC 1. First select column is "daily.rain_sum" and apply the explode function on this source column and also add alias for exploded values column as "rainFall"
# MAGIC 1. Along with above explode select the columns "marketName" , "latitude" , "longitude" from source Spark Dataframe
# MAGIC 1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'
# MAGIC 1. Add one more column from the Source Spark Dataframe "daily_units.rain_sum" and provide alias name as "unitOfRainFall"
# MAGIC 1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing

# COMMAND ----------

weatherDataRainFallTransDF = (weatherDataBronzeDF
                          .select(
                          explode("daily.rain_sum").alias("rainFall")
                          ,col("marketName")
                          ,col("latitude").alias("latitude")
                          ,col("longitude").alias("longitude")                          
                          ,monotonically_increasing_id().alias('sequenceId')
                          ,col("daily_units.rain_sum").alias("unitOfRainFall")

                          ))

display(weatherDataRainFallTransDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 7: Join All Intermediate Dataframes To Merge All Data & Write Into Silver Layer
# MAGIC
# MAGIC 1. Define New Spark Dataframe variable name as weatherDataTransDF
# MAGIC 1. Join weatherDataDailyDateTransDF with weatherDataMaxTemparatureTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']
# MAGIC 1. Extend weatherDataDailyDateTransDF with weatherDataMinTemparatureTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']
# MAGIC 1. Extend weatherDataDailyDateTransDF with weatherDataRainFallTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']
# MAGIC 1. Select the Columns "marketName" , "weatherDate" , "unitOfTemparature" , "maximumTemparature" , "minimumTemparature" , "unitOfRainFall" , "rainFall" , "latitude" and "longitude" to write final output columns into silve layer

# COMMAND ----------

weatherDataTransDF = (weatherDataDailyDateTransDF
                      .join(weatherDataMaxTemparatureTransDF, ['marketName','latitude','longitude','sequenceId'])
                      .join(weatherDataMinTemparatureTransDF, ['marketName','latitude','longitude','sequenceId'])
                      .join(weatherDataRainFallTransDF, ['marketName','latitude','longitude','sequenceId'])
                      .select(col("marketName")
                              ,col("weatherDate")
                              ,col("unitOfTemparature")
                              ,col("maximumTemparature")
                              ,col("minimumTemparature")
                              ,col("unitOfRainFall")
                              ,col("rainFall")
                              ,col("latitude")
                              ,col("longitude"))
                     
)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 8: Write the Final Transformed Dataframe Into Silve Layer As Delta Table
# MAGIC
# MAGIC 1. Write Final Spark Dataframe weatherDataTransDF values using spark.write method
# MAGIC 1. Use Write mode as overwrite 
# MAGIC 1. Write the data into the Datalake Table "pricing_analytics.silver.weather_data_silver" using saveAsTable Method

# COMMAND ----------

(weatherDataTransDF 
.write
.mode('overwrite')
.saveAsTable("pricing_analytics.silver.weather_data_silver"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 9: Test The Data Stored in Tranformed Silve Layer Table
# MAGIC 1. Write SELECT query to select the data from pricing_analytics.silver.weather_data_silver table
# MAGIC 1. Check the data for any one of the Market matches with the source data in Complex JSON format

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM  pricing_analytics.silver.weather_data_silver